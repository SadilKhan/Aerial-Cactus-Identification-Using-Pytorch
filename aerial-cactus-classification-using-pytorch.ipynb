{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms,datasets\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset,ConcatDataset\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.pooling import AvgPool3d\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../input/train.csv')\n",
    "sample=pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  has_cactus\n",
       "0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
       "1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
       "2  000d1e9a533f62e55c289303b072733d.jpg           1\n",
       "3  0011485b40695e9138e92d0b3fb55128.jpg           1\n",
       "4  0014d7a11e90b62848904c1418fc8cf2.jpg           1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17500 entries, 0 to 17499\n",
      "Data columns (total 2 columns):\n",
      "id            17500 non-null object\n",
      "has_cactus    17500 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 273.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13136\n",
       "0     4364\n",
       "Name: has_cactus, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['has_cactus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extra=train[train.has_cactus==0]\\ntrain=pd.concat([train,extra],axis=0)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"extra=train[train.has_cactus==0]\n",
    "train=pd.concat([train,extra],axis=0)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms={\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=0),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],\n",
    "                            [0.229,0.224,0.225])]),\n",
    "    'test':transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=0),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],\n",
    "                            [0.229,0.224,0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,val_set=train_test_split(train,stratify=train.has_cactus,test_size=0.2)\n",
    "\n",
    "len1=len(train_set)\n",
    "len2=len(val_set)\n",
    "\n",
    "# Loading Data From Folders\n",
    "train_dir='train'\n",
    "test_dir='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_(torch.utils.data.Dataset):\n",
    "    def __init__(self,labels,data_directory,transform):\n",
    "        super().__init__()\n",
    "\n",
    "        #characterizes a dataset for Pytorch\n",
    "        \n",
    "        self.list_id=labels.values[:,0]\n",
    "        self.labels=labels.values[:,1]\n",
    "        self.data_dir=data_directory\n",
    "        self.transform=transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Denotes the tota number of samples\n",
    "        return len(self.list_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        name=self.list_id[index]\n",
    "        img=Image.open('../input/train/train/{}'.format(name))\n",
    "        img=self.transform(img)\n",
    "        return img,torch.tensor(self.labels[index],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=dataset_(train_set,train_dir,image_transforms['train'])\n",
    "#train_df=DataLoader(train_df,batch_size=120,shuffle=True)\n",
    "\n",
    "val_set=dataset_(val_set,train_dir,image_transforms['test'])\n",
    "#val_df=DataLoader(val_df,batch_size=120,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst,labels=next(iter(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, tensor(0.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst.dtype,labels.dtype,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size(image_size,ker,stri,pad=0):\n",
    "    return (image_size-ker+2*pad)/stri +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=100,kernel_size=3)\n",
    "        self.conv2=nn.Conv2d(in_channels=100,out_channels=150,kernel_size=3)\n",
    "        self.conv3=nn.Conv2d(in_channels=150,out_channels=250,kernel_size=3)\n",
    "        #self.conv4=nn.Conv2d(in_channels=16,out_channels=20,kernel_size=3)\n",
    "\n",
    "        self.d1=nn.Dropout(0.3)\n",
    "        self.fc1=nn.Linear(in_features=250*4*4,out_features=500)\n",
    "        self.fc2=nn.Linear(in_features=500,out_features=200)\n",
    "        self.out=nn.Linear(in_features=200,out_features=2)\n",
    "\n",
    "\n",
    "    def forward(self,t):\n",
    "        #input layer\n",
    "        t=t\n",
    "        #first convolutional layer\n",
    "        t=F.max_pool2d(F.relu(self.conv1(t)),kernel_size=2,stride=2)\n",
    "        \n",
    "        #Second Convolutional Layer\n",
    "        \n",
    "        t=F.max_pool2d(F.relu(self.conv2(t)),stride=1,kernel_size=3)\n",
    "        \n",
    "        #Third Convolution Layer\n",
    "        \n",
    "        t=F.max_pool2d(F.relu(self.conv3(t)),stride=2,kernel_size=3)\n",
    "        \n",
    "        #Fourth Convolutional Layer\n",
    "        #t=F.max_pool2d(F.relu(self.conv4(t)),stride=3,kernel_size=3)\n",
    "        \n",
    "        # First linear Layer\n",
    "        t=t.reshape(-1,250*4*4)\n",
    "        t=F.relu(self.fc1(t))\n",
    "        t=self.d1(t)\n",
    "        \n",
    "        #Second Linear Layer\n",
    "        t=F.relu(self.fc2(t))\n",
    "        t=self.d1(t)\n",
    "        \n",
    "        #Third Linear Layer\n",
    "        t=self.out(t)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t train_loss 37.909471310675144 \t val_loss 5.014248371124268\n",
      "\n",
      "Epoch 2\t train_loss 17.54711987823248 \t val_loss 5.381856918334961\n",
      "\n",
      "Epoch 3\t train_loss 11.734003443270922 \t val_loss 2.42157244682312\n",
      "\n",
      "Epoch 4\t train_loss 9.560369810089469 \t val_loss 1.8283510208129883\n",
      "\n",
      "Epoch 5\t train_loss 6.624355269595981 \t val_loss 1.8180549144744873\n",
      "\n",
      "Epoch 6\t train_loss 6.935572700574994 \t val_loss 1.418682336807251\n",
      "\n",
      "Epoch 7\t train_loss 5.137770913541317 \t val_loss 1.3786144256591797\n",
      "\n",
      "Epoch 8\t train_loss 4.154282038565725 \t val_loss 1.4015976190567017\n",
      "\n",
      "Epoch 9\t train_loss 3.796827021986246 \t val_loss 1.113948941230774\n",
      "\n",
      "Epoch 10\t train_loss 3.5157620566897094 \t val_loss 1.1843180656433105\n",
      "\n",
      "Epoch 11\t train_loss 3.5170694766566157 \t val_loss 1.1555947065353394\n",
      "\n",
      "Epoch 12\t train_loss 2.9657405376201496 \t val_loss 1.1343905925750732\n",
      "\n",
      "Epoch 13\t train_loss 2.302657369989902 \t val_loss 0.9347913861274719\n",
      "\n",
      "Epoch 14\t train_loss 2.7122887826990336 \t val_loss 0.8577979803085327\n",
      "\n",
      "Epoch 15\t train_loss 2.1488608740037307 \t val_loss 0.8008976578712463\n",
      "\n",
      "Epoch 16\t train_loss 1.8655278788646683 \t val_loss 0.7802836894989014\n",
      "\n",
      "Epoch 17\t train_loss 1.7156047808239236 \t val_loss 0.9473536014556885\n",
      "\n",
      "Epoch 18\t train_loss 1.8572247282136232 \t val_loss 0.6521540284156799\n",
      "\n",
      "Epoch 19\t train_loss 1.7867599895689636 \t val_loss 0.5887504816055298\n",
      "\n",
      "Epoch 20\t train_loss 6.248603223939426 \t val_loss 1.2807049751281738\n",
      "\n",
      "Epoch 21\t train_loss 3.0339237722801045 \t val_loss 1.3638449907302856\n",
      "\n",
      "Epoch 22\t train_loss 2.3820932102389634 \t val_loss 0.887993574142456\n",
      "\n",
      "Epoch 23\t train_loss 1.7399691212922335 \t val_loss 0.8699740171432495\n",
      "\n",
      "Epoch 24\t train_loss 1.787777382764034 \t val_loss 0.7686087489128113\n",
      "\n",
      "Epoch 25\t train_loss 1.405817856430076 \t val_loss 1.1971147060394287\n",
      "\n",
      "Epoch 26\t train_loss 1.6898838707711548 \t val_loss 0.7733286619186401\n",
      "\n",
      "Epoch 27\t train_loss 1.131918807310285 \t val_loss 0.640450119972229\n",
      "\n",
      "Epoch 28\t train_loss 0.956131674873177 \t val_loss 0.7320086359977722\n",
      "\n",
      "Epoch 29\t train_loss 1.0214902088046074 \t val_loss 0.748310387134552\n",
      "\n",
      "Epoch 30\t train_loss 0.7478543462348171 \t val_loss 0.8601402044296265\n",
      "\n",
      "Epoch 31\t train_loss 0.8486533529794542 \t val_loss 1.0527358055114746\n",
      "\n",
      "Epoch 32\t train_loss 0.46675843924458604 \t val_loss 0.7245219945907593\n",
      "\n",
      "Epoch 33\t train_loss 0.6703269085701322 \t val_loss 0.6357777118682861\n",
      "\n",
      "Epoch 34\t train_loss 0.43366785733087454 \t val_loss 0.6406412720680237\n",
      "\n",
      "Epoch 35\t train_loss 0.3185096674860688 \t val_loss 1.9854414463043213\n",
      "\n",
      "Epoch 36\t train_loss 0.5908430300769396 \t val_loss 0.7593626379966736\n",
      "\n",
      "Epoch 37\t train_loss 0.5910828575797495 \t val_loss 0.9021818041801453\n",
      "\n",
      "Epoch 38\t train_loss 0.2973871836438775 \t val_loss 0.7938860058784485\n",
      "\n",
      "Epoch 39\t train_loss 0.26113479966079467 \t val_loss 0.6683218479156494\n",
      "\n",
      "Epoch 40\t train_loss 0.15364011316705728 \t val_loss 0.7452263832092285\n",
      "\n",
      "Epoch 41\t train_loss 0.19024579724282376 \t val_loss 0.7975966334342957\n",
      "\n",
      "Epoch 42\t train_loss 0.22326235720174736 \t val_loss 1.0468477010726929\n",
      "\n",
      "Epoch 43\t train_loss 25.478603286246653 \t val_loss 2.575667142868042\n",
      "\n",
      "Epoch 44\t train_loss 6.071243855170906 \t val_loss 1.4733806848526\n",
      "\n",
      "Epoch 45\t train_loss 3.3122706315480173 \t val_loss 0.9460997581481934\n",
      "\n",
      "Epoch 46\t train_loss 2.2143357649911195 \t val_loss 0.8814740180969238\n",
      "\n",
      "Epoch 47\t train_loss 1.8709269675891846 \t val_loss 1.1943416595458984\n",
      "\n",
      "Epoch 48\t train_loss 1.4372765829320997 \t val_loss 0.6024736762046814\n",
      "\n",
      "Epoch 49\t train_loss 1.1718769891303964 \t val_loss 0.7337252497673035\n",
      "\n",
      "Epoch 50\t train_loss 1.439417831483297 \t val_loss 0.7038532495498657\n",
      "\n",
      "Epoch 51\t train_loss 0.9790035767364316 \t val_loss 0.6916606426239014\n",
      "\n",
      "Epoch 52\t train_loss 0.7063891951402184 \t val_loss 0.6006804704666138\n",
      "\n",
      "Epoch 53\t train_loss 0.8494532212353079 \t val_loss 0.7028342485427856\n",
      "\n",
      "Epoch 54\t train_loss 0.979706619749777 \t val_loss 0.8178468942642212\n",
      "\n",
      "Epoch 55\t train_loss 0.34210055466610356 \t val_loss 0.5975443124771118\n",
      "\n",
      "Epoch 56\t train_loss 0.48844361715600826 \t val_loss 0.5990711450576782\n",
      "\n",
      "Epoch 57\t train_loss 0.43406904857693007 \t val_loss 0.9403578042984009\n",
      "\n",
      "Epoch 58\t train_loss 0.27356021201921976 \t val_loss 0.6574608087539673\n",
      "\n",
      "Epoch 59\t train_loss 0.2402816393368994 \t val_loss 0.8025853633880615\n",
      "\n",
      "Epoch 60\t train_loss 0.15538082790590124 \t val_loss 0.7431314587593079\n",
      "\n",
      "Epoch 61\t train_loss 0.12180112640271545 \t val_loss 0.73856121301651\n",
      "\n",
      "Epoch 62\t train_loss 0.17102885426356806 \t val_loss 0.9392648935317993\n",
      "\n",
      "Epoch 63\t train_loss 0.32299604707441176 \t val_loss 0.798930823802948\n",
      "\n",
      "Epoch 64\t train_loss 1.9284770794583892 \t val_loss 0.9323832392692566\n",
      "\n",
      "Epoch 65\t train_loss 0.690585563890636 \t val_loss 0.8392856121063232\n",
      "\n",
      "Epoch 66\t train_loss 0.22580755119997775 \t val_loss 0.6944930553436279\n",
      "\n",
      "Epoch 67\t train_loss 0.4910839071817463 \t val_loss 0.9716177582740784\n",
      "\n",
      "Epoch 68\t train_loss 0.0988672501880501 \t val_loss 0.7203341722488403\n",
      "\n",
      "Epoch 69\t train_loss 0.06125945843086811 \t val_loss 0.8312140703201294\n",
      "\n",
      "Epoch 70\t train_loss 0.04245932960657228 \t val_loss 0.8197866678237915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_sizes=130\n",
    "lrs=0.1\n",
    "train_loss=[]\n",
    "val_loss=[]\n",
    "train_correct=[]\n",
    "val_correct=[]\n",
    "epoch=[]\n",
    "model=Model()\n",
    "model=model.to('cuda:0')\n",
    "optimizer=optim.SGD(model.parameters(),lr=lrs)\n",
    "    \n",
    "train_df=DataLoader(train_set,batch_size=batch_sizes,shuffle=True)\n",
    "val_df=DataLoader(val_set,batch_size=batch_sizes,shuffle=True)\n",
    "\n",
    "for i in range(70):\n",
    "    total_loss=0 # Total loss for an epoch\n",
    "    total_correct=0 #Total correct number of predictions\n",
    "    for batch in train_df:\n",
    "        images,labels=batch #loading a batch\n",
    "\n",
    "        images=images.to('cuda:0') #changing the device\n",
    "        labels=labels.to('cuda:0') \n",
    "\n",
    "        preds=model(images) #predicting the probabilities of being in a class\n",
    "        labels=labels.long() #changing data type\n",
    "        f=nn.Sigmoid()\n",
    "        #preds=f(preds)\n",
    "\n",
    "        loss=F.cross_entropy(preds,labels) #CALCULATING lOSS\n",
    "            #changing the earlier gradient\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward() # Calculating the gradient\n",
    "        optimizer.step()# Updating the gradient\n",
    "        total_loss+=loss.item() # calculating the total loss \n",
    "        total_correct+=preds.argmax(dim=1).eq(labels).sum().item()#Calculating the correct prediction in a single epoch\n",
    "        del images,labels\n",
    "        \n",
    "    train_loss.append(total_loss)\n",
    "    train_correct.append(total_correct/len1)\n",
    "    epoch.append(i+1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_val_loss=0\n",
    "        total_val_correct=0\n",
    "        for val_batch in val_df:\n",
    "            val_im,val_lab=val_batch\n",
    "            val_im=val_im.to('cuda:0')\n",
    "            val_lab=val_lab.to('cuda:0')\n",
    "\n",
    "            val_preds=model(val_im)\n",
    "            val_lab=val_lab.long()\n",
    "            #val_preds=f(val_preds)\n",
    "            \n",
    "            loss_val=F.cross_entropy(val_preds,val_lab)\n",
    "            total_val_loss+=loss_val\n",
    "            total_val_correct+=val_preds.argmax(dim=1).eq(val_lab).sum().item()\n",
    "\n",
    "        val_loss.append(total_val_loss)\n",
    "        val_correct.append(total_val_correct/len2)\n",
    "\n",
    "        print(\"Epoch {}\\t train_loss {} \\t val_loss {}\\n\".format(epoch[i],total_loss,total_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9937142857142857,\n",
       "  0.9922857142857143,\n",
       "  0.9931428571428571,\n",
       "  0.9925714285714285,\n",
       "  0.9942857142857143],\n",
       " [0.9995714285714286, 0.9987857142857143, 1.0, 1.0, 1.0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_correct[-5:],train_correct[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans=transforms.ToTensor()\n",
    "submissions=pd.DataFrame({'id':[],'has_cactus':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(4000):\n",
    "        im=Image.open('../input/test/test/{}'.format(sample.iloc[i][0]))\n",
    "        im=image_transforms['test'](im)\n",
    "        im=im.to('cuda:0')\n",
    "        im=im.unsqueeze(dim=0)\n",
    "        test_pred=model(im)\n",
    "        #test_pred=f(test_pred)\n",
    "        test_pred=test_pred.argmax(dim=1).sum().item()\n",
    "        submissions=pd.concat([submissions,pd.DataFrame({'id':[sample.iloc[i][0]],\n",
    "                                                                 'has_cactus':[test_pred]})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    2904\n",
       "0.0    1096\n",
       "Name: has_cactus, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions['has_cactus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.to_csv('samplesubmission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
